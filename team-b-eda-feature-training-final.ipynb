{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h2><center><strong>Uniquant Market Prediction -  Team B</strong></center></h2>\n<h3><center><strong>Introduction</strong></center></h3>\n\nIn financial market, fluctuation are expected irrespective of the ivestment strategy used. However, investors still find ways to estimate their overall returns. Factors including the ivestment types have different risks and give different returns, which significantly impact the stability and volatility. There are many computer-based algorithms and models in financial market trading used to predict returns.\n\nUbiquant Investment (Beijing) Co., Ltd, one of the leading domestic quantitative hedge fund based in China was established in 2012. To drive quantitative financial market investment, Ubiquant rely on international talents in math and computer science together with cutting-edge technology. In general, Ubiquant is committed to creating long-term stable returns for investors.\n\n<h3><center><strong>Problem Statement</strong></center></h3>\nThe goal of Ubiquant Market Competition is to predict the future return of an asset given features over a specific period. All the elements in the dataset are anonymized by applying a function which simulate the actual dataset. The period over which the observations were recorded is concealed. \nThe dataset consists of 1211 different time steps and 3500 assets. The time steps in which investment features were recorded differed, and some got less than two records. Setting up a prediction model out of this is challenging as 27 percent of information is missing. Filling the gaps with an interpolation algorithm is difficult as well. The targets or forward returns show a normal distribution, and do not appear to correlate with features. It is noteworthy that there are so many correlated features.\nThere are three ways to address this problem, and first is handling it as a regression task by predicting the targets using existing features. Second is factoring time and features to forecast targets or a combination of the first and second technique. \n\n<h3><center><strong>Goal</strong></center></h3>\n\nThe main goal of the competition is to build a model that forecasts an investment's return rate. In both the training and testing, the algorithm utilizes historical prices data. A well developed model will solve the real-world data science problem on return forecasting with a higher accuracy possible. \n\nThis will improve the ability of quantitative researchers to forecast returns. In addition, it will enable investors at any scale to make better decisions.\n\n\n<h3><center><strong>Metrics</strong></center></h3>\nEvaluation of the model is based on the mean of the Pearson correlation coefficient for each time ID. The submistion included the use of a time-series API, to ensures the model does not peek forward in time. An error could occur if the submission includes nulls or infinities and the submissions that only include one prediction value would receive a score of -1.\n\nSince this is a forecasting competition with an active training phase,their will be a second period where models will be run \n\nThis will improve the ability of quantitative researchers to forecast returns. In addition, it will enable investors at any scale to make better decisions.\n<h3><center><strong>Methodology</strong></center></h3>\n\nThe first task of this challenge was modelling a minimum viable product that would predict the targets based on existing features. Team members agreed to work on this task individually, and make a submission. The final model for submission was based on the performances of the models, it's complexity, computational demands and difficulty.\n\n<h4><strong>Random Forest Regressor - Rhodasi</strong></h4>\n\nA Random Forest Regressor was implemented using Keras module in Tensorflow. The model is best suited for classification tasks with multiple features. More so Random Forest Regressor is robust to outliers, noise and variance. The model was trained on 30 percent of the dataset and achieved and  accuracy of 0.021. Due to the size of the dataset, the tree depth is limited by compute resources which reduces model performance.\n\n<h4><strong>Vector Autoregression - Lali</strong></h4>\n\n<h4><strong>A DNN with Leaky ReLu - Mercelina</strong></h4>\nDeep learning models are able to automatically learn arbitrary complex mappings from inputs to outputs.\n\nA deep learning model with LeakyReLU activation was used as an MVP. During training, loss optimization was Adam with a learning rate of 0.003. The training was conducted for 5 epochs and gave a score of 0.142.  Maintaining 5 epochs with a learning rate of 0.001 improved the score to 0.143. Adjusting the learning rate to 0.001, and training epochs to 20, a score of 0.146 was attained. Further increase in epochs to 30, lead to increase in validation loss due to overfiting, and the score reduced to 0.144. Using early stopping and saving the best model did not improve the model score from 0.146.\n\n<h4><strong>A Dense Convolution Neural Network -  Dennies</strong></h4>\n\nA convolution layer as a feature engineering technique was used to reduce dimensionality. A single channel image of 1 by 300 dimensions is the input to the network. The convolution layer has a 3 by 3 filter, and max pooling layers with a stride of 2. The output of the layer is then fed into a dense layer. The convolution layer reduces the input dimensions while retaining key features.\nThe data set was split into five folds, and which were then further subdivided into the training and validation set. During training, early stopping was used to decrease over-fitting when validation loss rose. The root mean square of the training set was 0.91 and the Pearson score was 0.139 on the leaderboard.\nConsidering the computational demands and low performance despite various optimization techniques, the model was discarded for the other fronted models.\n<h3><center><strong>Selected Model - </strong></center></h3>\nTeam members evaluated their mvps and agreed on working with Arnold's model based on dense neural network and ensemble learning. The leaderboard score was 0.149, outperforming the other fronted models. The model employs ensemble techniques which allows for improvement. The team worked on the model to improve its performance and achieved a maximum score of 0.153.\nThe next section breaks down into the EDA, feature engineering, model training and submission\n\n","metadata":{}},{"cell_type":"markdown","source":"## **Exploratory Data Analysis**\nWe explore the structure of the data, its characteristics, and relationships","metadata":{}},{"cell_type":"code","source":"import os\n\n# exploratory data analysis\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom itertools import cycle\nfrom scipy import stats\nimport seaborn as sns\n\n# model training\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nfrom tensorflow.python.keras import backend as K\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\n\n# feature engineering\n\n\nplt.style.use(\"ggplot\")\ncolor_pal = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\ncolor_cycle = cycle(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-03-06T18:01:35.791030Z","iopub.execute_input":"2022-03-06T18:01:35.791303Z","iopub.status.idle":"2022-03-06T18:01:35.800494Z","shell.execute_reply.started":"2022-03-06T18:01:35.791274Z","shell.execute_reply":"2022-03-06T18:01:35.799444Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### **Load the Dataset**.\n<p> The pickle file is adapted from <a href=\"https://www.kaggle.com/lonnieqin/ubiquant-market-prediction-half-precision-pickle\"><strong>Lonnie</strong></a> which is a reduced version of the original dataset. This speeds up loading and minimizes memory exhaustion</p>","metadata":{}},{"cell_type":"code","source":"# train df\ntrain = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\n\n# convert the train as float16\n\ntrain = train.astype('float16')","metadata":{"execution":{"iopub.status.busy":"2022-03-06T18:01:48.510070Z","iopub.execute_input":"2022-03-06T18:01:48.510361Z","iopub.status.idle":"2022-03-06T18:02:09.332203Z","shell.execute_reply.started":"2022-03-06T18:01:48.510333Z","shell.execute_reply":"2022-03-06T18:02:09.330964Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T15:58:01.283526Z","iopub.execute_input":"2022-03-06T15:58:01.284233Z","iopub.status.idle":"2022-03-06T15:58:01.332573Z","shell.execute_reply.started":"2022-03-06T15:58:01.28418Z","shell.execute_reply":"2022-03-06T15:58:01.331664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The train dataframe has 300 anonymized features labelled f_0 to f_299, investments and targets at different times.","metadata":{}},{"cell_type":"code","source":"#  extracting the features of the df\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]","metadata":{"execution":{"iopub.status.busy":"2022-03-06T15:58:01.334181Z","iopub.execute_input":"2022-03-06T15:58:01.334448Z","iopub.status.idle":"2022-03-06T15:58:01.339255Z","shell.execute_reply.started":"2022-03-06T15:58:01.334415Z","shell.execute_reply":"2022-03-06T15:58:01.338048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Are there any nulls? What are the total number of observations?**\n<a id=\"1\"></a>","metadata":{}},{"cell_type":"code","source":"#  check the shape and NaNS\n\nobs = train.shape[0]\nprint(f\"number of observations: {obs}\")\nprint(f\"NaNs in the dataframe? {str(train.isnull().values.any())}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-06T15:58:01.341914Z","iopub.execute_input":"2022-03-06T15:58:01.342284Z","iopub.status.idle":"2022-03-06T15:58:06.117333Z","shell.execute_reply.started":"2022-03-06T15:58:01.342226Z","shell.execute_reply":"2022-03-06T15:58:06.115738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Investments & Observation size (Time)**\n<a id='time_investments'></a>\n\nAll the investments have a total of 3141410 observations from the [cell](#1) above. We investigate if these observations are distributed equally among the investments. ","metadata":{}},{"cell_type":"code","source":"# the number of investment ids, and max available time ids\n# the investment_id with highest number of timesteps\n\nobs_by_asset = train.groupby(['investment_id'])['target'].count()\n# number of unique investments\nassets = train.investment_id.nunique()\ntime_steps =train.time_id.nunique()\nprint(f\"number of investments: {assets}\\nmax time steps: {time_steps}\")\nprint(f\"highest observation window: investement_id {np.argmax(obs_by_asset)} with {np.max(obs_by_asset)}\")\nprint(f\"lowest observation window: investement_id {np.argmin(obs_by_asset)} with {np.min(obs_by_asset)}\")\n","metadata":{"execution":{"iopub.status.busy":"2022-03-06T15:58:06.119994Z","iopub.execute_input":"2022-03-06T15:58:06.120369Z","iopub.status.idle":"2022-03-06T15:58:06.323002Z","shell.execute_reply.started":"2022-03-06T15:58:06.120327Z","shell.execute_reply":"2022-03-06T15:58:06.321743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There  are 3579 investments. Investment 1334 has the lowest observation time of 2 whereas investment 2027 has highest observation time of 1211.\nInvestment have different observation times and therefore the dataset is not balanced.\nSome of the investment_id's are underepresented in the datset. We may attempt to remove them to see if our model's accuracy and/or predictive power will improve","metadata":{}},{"cell_type":"markdown","source":"#### **Quartile Distribution**","metadata":{}},{"cell_type":"code","source":"# finding the quartiles of the time steps \n\nobs_by_asset.quantile([0.25,0.5,0.75])\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-06T15:58:06.324718Z","iopub.execute_input":"2022-03-06T15:58:06.325087Z","iopub.status.idle":"2022-03-06T15:58:06.339922Z","shell.execute_reply.started":"2022-03-06T15:58:06.325037Z","shell.execute_reply":"2022-03-06T15:58:06.338633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"50% of the investments have time steps between 0 and 683 for lower quartile and 1131 and 1211 for Q3.\nThe rest are between median value of 1009 and Q3","metadata":{}},{"cell_type":"code","source":"# visual rep of the sizes of observations\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\nobs_by_asset.plot.hist(bins=60)\nplt.title(\"timesteps by asset distribution\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T15:58:06.342453Z","iopub.execute_input":"2022-03-06T15:58:06.342965Z","iopub.status.idle":"2022-03-06T15:58:06.759302Z","shell.execute_reply.started":"2022-03-06T15:58:06.342902Z","shell.execute_reply":"2022-03-06T15:58:06.758473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The sample size of each investment is equal to the time steps used in recording the data. As computed in the previous cell, the majority of the investments have observations between 800 and 1200\n<a id=\"2\"></a>","metadata":{}},{"cell_type":"code","source":"# we investigate the distribution of each investment observation across the min and the max time steps\n\ntrain[['investment_id', 'time_id']].plot.scatter('time_id', 'investment_id', figsize=(20, 30), s=0.5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T15:58:06.76058Z","iopub.execute_input":"2022-03-06T15:58:06.761587Z","iopub.status.idle":"2022-03-06T15:58:11.393669Z","shell.execute_reply.started":"2022-03-06T15:58:06.761526Z","shell.execute_reply":"2022-03-06T15:58:11.392749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The white patches shows missing data at the specific time id. The are some vertical white lines which are common across the various observations. These are probably holidays, missing or removed data.","metadata":{}},{"cell_type":"markdown","source":"### **Targets && Investment Ids**\n<a id='targets_investments'></a>\n\nWe average the targets of each investment across the observation window. For each investment also, the standard deviation of targets is calculated.","metadata":{}},{"cell_type":"code","source":"# The mean of the targets of each asset\nmean_target = train.groupby(['investment_id'])['target'].mean()\n\n# mean of the mean of each investment\nmean_mean_target =mean_target.mean()\nkde = stats.gaussian_kde(mean_target)\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\nmean_target.hist(bins=60)\nplt.title(\"mean target distribution\")\nplt.show()\n\nprint(f\"Mean of mean target: {mean_mean_target: 0.5f}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-06T15:58:11.395264Z","iopub.execute_input":"2022-03-06T15:58:11.395759Z","iopub.status.idle":"2022-03-06T15:58:11.836455Z","shell.execute_reply.started":"2022-03-06T15:58:11.395708Z","shell.execute_reply":"2022-03-06T15:58:11.835502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution of targets is normal with a mean of almost zero. There are no outliers causing long tails, and can be used for modelling without further preprocessing","metadata":{}},{"cell_type":"code","source":"# the standard deviation of the tragets of each investment id\n\nsts_target = train.groupby(['investment_id'])['target'].std()\n\n# the mean of the std of the targets\nmean_std_target = np.mean(sts_target)\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\nsts_target.plot.hist(bins=60)\nplt.title(\"standard deviation of target distribution\")\nplt.show()\n\nprint(f\"Mean of std target: {mean_std_target: 0.5f}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-06T15:58:11.83948Z","iopub.execute_input":"2022-03-06T15:58:11.840205Z","iopub.status.idle":"2022-03-06T15:58:12.263554Z","shell.execute_reply.started":"2022-03-06T15:58:11.840146Z","shell.execute_reply":"2022-03-06T15:58:12.262555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The targets have a mean standard deviation of 0.9. Approximately 95% of the investments have a mean std between 0.5 and 1.5. There are a few data points with high STD","metadata":{}},{"cell_type":"code","source":"#  A plot of the std and the mean of each each asset\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\nsts_target.plot(style='.-',color=next(color_cycle),legend=True,label='MEAN_STD')\nax.axhline(y=mean_std_target,linestyle='--',color=next(color_cycle))\nax.text(4000,mean_std_target+0.1,'mean std targets')\n\nax.text(4000,mean_mean_target+0.1,'mean mean targets')\nmean_target.plot(style='.-',color=next(color_cycle),legend=True,label='MEAN_MEAN')\nax.axhline(y=mean_mean_target,linestyle='--',color=next(color_cycle))\nplt.title(\"standard deviation & Mean of target distribution\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T15:58:12.264977Z","iopub.execute_input":"2022-03-06T15:58:12.265245Z","iopub.status.idle":"2022-03-06T15:58:12.625509Z","shell.execute_reply.started":"2022-03-06T15:58:12.265209Z","shell.execute_reply":"2022-03-06T15:58:12.624688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plot of mean of asset targets is about zero, and confirm distribution of targets as normal. There a few cases of spiking in STD, and probably the reason for outliers in the distribution plot.","metadata":{}},{"cell_type":"markdown","source":"### **Time and Targets**\nIn this section, we are utilizing the observation size of each investment from [Investments & Time](#time_investments) and [Investment Targets](#targets_investments) to visualize the relationship of the time and the targets. We use Seaborn Joint plot library to compare how the mean and standard deviation of each investment target vary across time","metadata":{}},{"cell_type":"code","source":"#  obs by target is a count of the time steps per investment id\n#  A plot of mean of targets for each investment id\n\nax = sns.jointplot(x=obs_by_asset.values, y=mean_target, kind=\"reg\", \n                   height=8, joint_kws={'line_kws':{'color':next(color_cycle)}})\nax.ax_joint.set_xlabel('count timesteps/investment')\nax.ax_joint.set_ylabel('mean target of investment')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T15:58:12.626949Z","iopub.execute_input":"2022-03-06T15:58:12.627964Z","iopub.status.idle":"2022-03-06T15:58:14.454674Z","shell.execute_reply.started":"2022-03-06T15:58:12.627907Z","shell.execute_reply":"2022-03-06T15:58:14.451759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The mean is constant for all the time ids. ","metadata":{}},{"cell_type":"code","source":"#  the count of investment time steps against mean std of each investment\n\nqx = sns.jointplot(x=obs_by_asset.values, y=sts_target, kind=\"reg\", \n                   height=8, joint_kws={'line_kws':{'color':next(color_cycle)}})\nax.ax_joint.set_xlabel('observations')\nax.ax_joint.set_ylabel('std target')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T15:58:14.4561Z","iopub.execute_input":"2022-03-06T15:58:14.456369Z","iopub.status.idle":"2022-03-06T15:58:15.537975Z","shell.execute_reply.started":"2022-03-06T15:58:14.456339Z","shell.execute_reply":"2022-03-06T15:58:15.536944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Time affects the mean standard deviation of the targets. It's obvious that the standard deviation reduces with increasing time steps\nThis is in contrast to the mean which is constant through the observation space.","metadata":{}},{"cell_type":"markdown","source":"#### **Is there any correlation between time and targets?**","metadata":{}},{"cell_type":"code","source":"#  we find the correlation of the time id and the targets\n\nr = np.corrcoef(train.groupby('time_id')['investment_id'].nunique(), train.groupby('time_id')['target'].mean())[0][1]\n\nprint(f\"Correlation of number of assets by target: {r:0.3f}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-06T15:58:15.539721Z","iopub.execute_input":"2022-03-06T15:58:15.539992Z","iopub.status.idle":"2022-03-06T15:58:16.317427Z","shell.execute_reply.started":"2022-03-06T15:58:15.539961Z","shell.execute_reply":"2022-03-06T15:58:16.316342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The coefficient is negative, and almost zero.Therefore, it can be concluded that there is no correlation.\nIn the next section, we will plot the mean and std of all the investments at different time windows.","metadata":{}},{"cell_type":"code","source":"time2target_mean = train.groupby(['time_id'])['target'].mean() #mean of targets at specific time ids\ntime2target_std = train.groupby(['time_id'])['target'].std()  #std of targets at each time id\n\n_, axes = plt.subplots(1, 1, figsize=(24, 12))\n\n# fills the region between the minimum mean and maximum mean of the investments\nplt.fill_between(\n        time2target_mean.index,\n        time2target_mean - time2target_std,\n        time2target_mean + time2target_std,\n        alpha=0.1,\n        color=\"b\",\n    )\n\nplt.plot(\n        time2target_mean.index, time2target_mean, \"o-\", color=next(color_cycle), label=\"Training score\"\n    )\nplt.axhline(y=mean_mean_target, color=next(color_cycle), linestyle='--', label=\"mean\")\nplt.title(\"Mean Targets vs TimeIDs\")\naxes.set_ylabel(\"target\")\naxes.set_xlabel(\"time\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T15:58:16.320073Z","iopub.execute_input":"2022-03-06T15:58:16.320516Z","iopub.status.idle":"2022-03-06T15:58:16.793419Z","shell.execute_reply.started":"2022-03-06T15:58:16.320469Z","shell.execute_reply":"2022-03-06T15:58:16.792221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is spiking in average of the mean between time id 380 to 500 compared to other regions where the target mean appear to be uniform. In this region, probably, there are missing data points for the majority of the investment ids. The white patches in the scatter plot [here](#2) concides with the spikes in this plot. It's the evidence of missing data.","metadata":{}},{"cell_type":"code","source":"# a random investment id is \ntime2target_mean = train.groupby(['time_id'])['target'].mean()\ntime2target_std = train.groupby(['time_id'])['target'].std()\n\n_, axes = plt.subplots(1, 1, figsize=(24, 12))\nplt.fill_between(\n        time2target_mean.index,\n        time2target_mean - time2target_std,\n        time2target_mean + time2target_std,\n        alpha=0.1,\n        color=\"b\",\n    )\nplt.plot(\n        time2target_mean.index, time2target_mean, \"o-\", color=next(color_cycle), label=\"time-target mean\"\n    )\nplt.axhline(y=mean_mean_target, color=next(color_cycle), linestyle='--', label=\"mean\")\n\nasset = np.random.randint(0,train.investment_id.nunique())\nplt.scatter(train[train.investment_id==asset].time_id,\n               train[train.investment_id==asset].target, color=next(color_cycle),label=f'investment_id {asset}')\nplt.legend()\n\naxes.set_ylabel(\"target\")\naxes.set_xlabel(\"time\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T15:58:16.795006Z","iopub.execute_input":"2022-03-06T15:58:16.795315Z","iopub.status.idle":"2022-03-06T15:58:17.450793Z","shell.execute_reply.started":"2022-03-06T15:58:16.795276Z","shell.execute_reply":"2022-03-06T15:58:17.449671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A random investment is scattered along the mean-target-time plot. For the next section, we inspect specific investments, and how time affects it's targets","metadata":{}},{"cell_type":"code","source":"# we are plotting the random investment ids against time ids\n\nrandom_investments = set(list(np.random.randint(0,300) for num in range(7)))\n\nfor investment_id in random_investments:\n    d = train.query('investment_id == @investment_id')\n    d.set_index('time_id')['target'] \\\n        .plot(figsize=(15, 5),\n              title=f'Investment_id {investment_id}',\n              color=next(color_cycle),\n              style='.-')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T15:58:17.452288Z","iopub.execute_input":"2022-03-06T15:58:17.452596Z","iopub.status.idle":"2022-03-06T15:58:19.466953Z","shell.execute_reply.started":"2022-03-06T15:58:17.452558Z","shell.execute_reply":"2022-03-06T15:58:19.465825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plots of random assets indicate missing data and gaps in the observation window. Some of them such as investment 262 has completely no data points. The missing points will impede model performance in the training section. Investment 109 is selected to demonstrate these data gaps","metadata":{}},{"cell_type":"code","source":"from matplotlib.patches import Ellipse\n# a plot of investment_id 59\ninvestment_id=109\n\nfig, ax = plt.subplots(1, 1,figsize=(15, 5))\nd = train.query('investment_id == @investment_id')\n\nd.set_index('time_id')['target'] \\\n    .plot(title=f'Investment_id {investment_id}',\n          color=next(color_cycle),\n          style='.-')\n\ncircle_rad = 30  # This is the radius, in points\ncircle_2 = 40\ncolor = next(color_cycle)\nax.plot(380, 0.5, 'o',\n        ms=circle_rad * 2, mec=color, mfc='none', mew=2)\nax.annotate('Missing Data', xy=(380,1), xytext=(400, 4),\n            color='b', size='large',\n            arrowprops=dict(\n                arrowstyle='simple,tail_width=0.3,head_width=0.8,head_length=0.8',\n                facecolor=color, shrinkB=circle_rad * 0.15))\nax.plot(1100, 1.2, 'o',\n        ms=circle_2 * 2, mec=color, mfc='none', mew=2)\nax.annotate('', xy=(1100,1.2), xytext=(450, 4),\n            color='b', size='large',\n            arrowprops=dict(\n                arrowstyle='simple,tail_width=0.3,head_width=0.8,head_length=0.8',\n                facecolor=color, shrinkB=circle_2 * 0.05))\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T15:58:19.468432Z","iopub.execute_input":"2022-03-06T15:58:19.468759Z","iopub.status.idle":"2022-03-06T15:58:20.182456Z","shell.execute_reply.started":"2022-03-06T15:58:19.468678Z","shell.execute_reply":"2022-03-06T15:58:20.181237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Features**\n\nThere are 300 features whose values are anonymized. In the first section, we will plot the distribution of 30 features ","metadata":{}},{"cell_type":"code","source":"def plot_hist():\n    '''Plot distribution of the features extracted randomly\n    between 0 & 299\n    \n    '''\n\n    columns = [f\"f_{_}\" for _ in np.random.choice(range(299), 30, replace=False)]\n    \n    train.hist(column = columns, bins = 100, figsize = (30,30))\n    plt.show()\n\n    return\n\nplot_hist()\n","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-06T15:58:20.185105Z","iopub.execute_input":"2022-03-06T15:58:20.185467Z","iopub.status.idle":"2022-03-06T15:58:46.345739Z","shell.execute_reply.started":"2022-03-06T15:58:20.185423Z","shell.execute_reply":"2022-03-06T15:58:46.344004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Its difficult to make sense of the histograms of the features. There are some which exhibit a normal distribution, some which are highly skewed with long tails, and multimodal(non-uniform) distribution.\nWhat is conspicous also is missing data points.","metadata":{}},{"cell_type":"markdown","source":"#### **features vs targets**\n>First, is trending the random features vs targets to check for trends or correlation\n<a id='3'></a>","metadata":{}},{"cell_type":"code","source":"#  Using 3M+ observations will cause memory lag. Observations are sampled\n\ntarget_sample = train.sample(frac = 0.10)\n\ndef target_feature():\n    \n    \"\"\"\n    This function randomly picks 30 features without replacement,\n    and plot against the target.\n    \"\"\"\n    \n    fig, ax = plt.subplots(6, 5, figsize=(30,30), sharey=True)\n    \n    axes = [axis for axis in ax.flatten()]\n    \n    for i, _ in enumerate(np.random.choice(range(299), 30, replace=False)):\n        ax_ = axes[i]\n\n        df_ = target_sample[['target',f'f_{_}']]\n        ax_.scatter(df_[f'f_{_}'], df_['target'], alpha=0.15)\n        ax_.get_xaxis().set_visible(False)\n        ax_.get_yaxis().set_visible(False)\n        ax_.axis('off')\n        plt.text(\n            0.1,\n            0.9,\n            f'f_{_}',\n            horizontalalignment='left',\n            verticalalignment='top',\n            transform = ax_.transAxes)\n        del df_\n    plt.show()\n\n# target_feature()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T16:38:07.623178Z","iopub.execute_input":"2022-03-06T16:38:07.623484Z","iopub.status.idle":"2022-03-06T16:38:10.095022Z","shell.execute_reply.started":"2022-03-06T16:38:07.623443Z","shell.execute_reply":"2022-03-06T16:38:10.094014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The scatter plots appear as noise, and there seems to be no visible trends among the features and targets. We are then going to compute the correlation of the features and targets.","metadata":{}},{"cell_type":"code","source":"# correlation of features && targets\n\ncorrelation = target_sample[['target']+features].corr()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T16:04:59.442818Z","iopub.execute_input":"2022-03-06T16:04:59.443045Z","iopub.status.idle":"2022-03-06T16:06:16.067443Z","shell.execute_reply.started":"2022-03-06T16:04:59.443018Z","shell.execute_reply":"2022-03-06T16:06:16.066622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# \nfig, ax = plt.subplots(1,1,figsize=(10, 4))\n\nplt.hist(correlation[\"target\"][1:], edgecolor = \"black\", bins = 25)\nax.set_ylabel(\"Frequency of  Corr\")\nax.set_xlabel(\"Target-Feature Correlation\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T16:06:16.068503Z","iopub.execute_input":"2022-03-06T16:06:16.068749Z","iopub.status.idle":"2022-03-06T16:06:16.3048Z","shell.execute_reply.started":"2022-03-06T16:06:16.068723Z","shell.execute_reply":"2022-03-06T16:06:16.30389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There seems to be a no strong linear correlation between the features and the target. Max positive corr is 0.04, and the min is -0.06. Feature-target plots [here](#3) had shown no trend  between the targets and features.","metadata":{}},{"cell_type":"code","source":"sns.clustermap(abs(correlation), figsize = (15,15), cmap = \"mako\");","metadata":{"execution":{"iopub.status.busy":"2022-03-06T16:06:16.306844Z","iopub.execute_input":"2022-03-06T16:06:16.307138Z","iopub.status.idle":"2022-03-06T16:06:18.438292Z","shell.execute_reply.started":"2022-03-06T16:06:16.307097Z","shell.execute_reply":"2022-03-06T16:06:18.437514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the linear correlation between the fx features, we can see that there are several highly correlated features. Highly correlated features often contain the same information and we could drop some features without information loss.","metadata":{}},{"cell_type":"markdown","source":"##### **Feature importance using correlations**","metadata":{}},{"cell_type":"code","source":"#Check if any of the features are at least moderately correlated with the target using pearson's r <-0.5 or >0.5\ncorrelations=[]\nfeature_columns=[]\n\nfor col in train.columns[2:-1]:\n    coef = np.corrcoef(train['target'], train[col])[0][1]\n    correlations.append(coef)\n    feature_columns.append(col)\n    \ntarget_feature_coeff=pd.concat([pd.Series(feature_columns), pd.Series(correlations)],axis=1)\n\n#rename the columns in our correlations_target df\ntarget_feature_coeff['features']=target_feature_coeff[0]\ntarget_feature_coeff['coeff']=target_feature_coeff[1]\ntarget_feature_coeff.drop([0,1], axis=1)\n\n#visualize the importance of features based on pearson's r\ni = target_feature_coeff['features']\nfeat_importances = pd.Series(correlations, index=i)\nfeat_importances.nlargest(100).plot(kind='barh', figsize=(20, 15)).invert_yaxis()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T16:15:08.29761Z","iopub.execute_input":"2022-03-06T16:15:08.297928Z","iopub.status.idle":"2022-03-06T16:15:25.186281Z","shell.execute_reply.started":"2022-03-06T16:15:08.297897Z","shell.execute_reply":"2022-03-06T16:15:25.185716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Checking whether the data is static or not**\nWe are testing whether the data is stationary or not using the Augmented Dickey-Fuller test (ADF Test). Stationarity refers to a series which doesn't exhibit seasonality or a trend. The data properties such as mean, variance, covariance are independent and not a function of time.\nADF is conducted with the following assumptions:\n* `Null Hypothesis` - *Data is non Stationary*\n* `Alternate Hypothesis` - *Data is stationary*\n\nNull hypothesis is adopted if the test static is less than the critical value, and p-value is less than 0.05\n","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\n\ndef adf_test(timeseries,asset):\n    print (f'Results of Dickey-Fuller Test: Investment_id{asset} \\n')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput,end='')\n    print('')","metadata":{"execution":{"iopub.status.busy":"2022-03-06T16:24:35.018462Z","iopub.execute_input":"2022-03-06T16:24:35.018772Z","iopub.status.idle":"2022-03-06T16:24:35.025653Z","shell.execute_reply.started":"2022-03-06T16:24:35.018737Z","shell.execute_reply":"2022-03-06T16:24:35.024675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For investment of id 70, we reject null hypothesis for alternate hypothesis. The time series is stationary from the p-value which is less than the significance level of 0.05, and the ADF statistic is less than any critical values","metadata":{}},{"cell_type":"code","source":"#  we perform ADF tests for more random investment ids\n\nfor i in range(5):\n    asset = np.random.randint(0,train.investment_id.nunique())\n    d = train.query('investment_id == @asset')\n    d = d.set_index('time_id')['target']\n    \n    \n    try:\n        adf_test(d,asset)\n    except:\n        print('err')\n        pass","metadata":{"execution":{"iopub.status.busy":"2022-03-06T16:24:37.118528Z","iopub.execute_input":"2022-03-06T16:24:37.118807Z","iopub.status.idle":"2022-03-06T16:24:37.757346Z","shell.execute_reply.started":"2022-03-06T16:24:37.118778Z","shell.execute_reply":"2022-03-06T16:24:37.756157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random investments tested above satisfy the ADF criteria for time stationarity. It can be concluded therefore that the assets in this dataset are stationary.","metadata":{}},{"cell_type":"markdown","source":"## **Feature Engineering**\n\nThis section outlines steps taken to transform the data in an attempt to improve model performance. The feature engineering tasks are based on exploratory data analysis section. [TIME SERIES ANALYSIS WITH PANDAS](https://ourcodingclub.github.io/tutorials/pandas-time-series/#resampling), [Fitting time series regression models](https://people.duke.edu/~rnau/timereg.html), and [6 Powerful Feature Engineering Techniques For Time Series Data (using Python)](https://www.analyticsvidhya.com/blog/2019/12/6-powerful-feature-engineering-techniques-time-series/) were referenced to guide the process.","metadata":{}},{"cell_type":"markdown","source":"### **Create Simulated DateTime Index**\n\nMost of the time series data transformations require a valid DateTime or PeriodTime index. Because the data does not specify one, the time_id column was used to create a dummy variable for Time indexing. It assumes a continous time-period with daily frequency.","metadata":{}},{"cell_type":"code","source":"def create_datetime(df,index):\n    \"\"\" Takes the time_id column of the dataframe to simulate a datetime object\n    \n     Args: a dataframe and the name of the index with the time-like data\n\n     Returns: dataframe with a datetime index\n    \n    \"\"\"\n    period=max(df[index]) - min(df[index]) + 2\n    dti=pd.date_range(start=\"2000-01-01\", periods=period, freq='D')\n    times=[time for time in range(period)]\n    data_index_dict=dict(zip(times, dti))\n    time_id=df[index].unique()\n    df['date_time']=df[index]\n    df['date_time'].replace(data_index_dict, inplace=True)\n    df['date_time'] =  pd.to_datetime(df['date_time']) \n    \ncreate_datetime(target_sample, 'time_id')\ntarget_sample = target_sample.set_index('date_time')\n#train.head()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T16:39:27.561838Z","iopub.execute_input":"2022-03-06T16:39:27.562359Z","iopub.status.idle":"2022-03-06T16:39:29.066273Z","shell.execute_reply.started":"2022-03-06T16:39:27.562325Z","shell.execute_reply":"2022-03-06T16:39:29.065382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Autocorrelation**","metadata":{}},{"cell_type":"code","source":"def random_autocorrelation_plot (data_frame, time_interval):\n    '''\n    plots the autocorrelation of 5 random features in a dataframe defined by a time interval\n    Args: \n    data_frame: pandas dataframe\n    time_interval: pandas datetime format e.g 'W','1w', '2w' 'Y','1y' etc - check pandas.DateTime documentation for full list\n    Returns: A graph of autocorrelation over specified time lag\n    '''\n    import random as r\n    random= [r.randint(0,300) for i in range(5)]\n    for i in random:\n        df = data_frame[f'f_{i}']\n        pd.plotting.autocorrelation_plot(df.resample(time_interval).median(), label=(f'feature {i}'))\n        plt.title(f\"Autocorrelation of feature f_{i} at interval {time_interval}\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T16:39:41.644293Z","iopub.execute_input":"2022-03-06T16:39:41.644581Z","iopub.status.idle":"2022-03-06T16:39:41.650949Z","shell.execute_reply.started":"2022-03-06T16:39:41.644547Z","shell.execute_reply":"2022-03-06T16:39:41.650049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Examine how autocorrelation of random features changes with size of our simulated lag times\nrandom_autocorrelation_plot (target_sample, '1w')\n\nrandom_autocorrelation_plot (target_sample, '1m')\n\nrandom_autocorrelation_plot (target_sample, '1y')","metadata":{"execution":{"iopub.status.busy":"2022-03-06T16:39:52.054815Z","iopub.execute_input":"2022-03-06T16:39:52.055343Z","iopub.status.idle":"2022-03-06T16:39:55.416047Z","shell.execute_reply.started":"2022-03-06T16:39:52.055289Z","shell.execute_reply":"2022-03-06T16:39:55.414947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Data Transformations**\n\nBecause the data has negative values, we were unable to use some popular mathematical transofrmations used for time-series data e.g log and squaring.","metadata":{}},{"cell_type":"markdown","source":"#### **Min-Max Scaling**\nThe model performance dropped from 0.149 to 0.137 with scaled data","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n#Normalize the data using Min-Max scaling\n\nsample_x = ['investment_id'] + [f'f_{i}' for i in range(300)]\n\nscaler = MinMaxScaler()\n\nscaled= scaler.fit_transform(target_sample[sample_x])\ngc.collect()\n\nscaled\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-06T16:40:04.29534Z","iopub.execute_input":"2022-03-06T16:40:04.296906Z","iopub.status.idle":"2022-03-06T16:40:06.065419Z","shell.execute_reply.started":"2022-03-06T16:40:04.296849Z","shell.execute_reply":"2022-03-06T16:40:06.064156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Lagged Dataframe**\nThe dataframe was lagged with a shift value of 1. Using a plot of autocorrelation lagging the data had the same effect as resampling. No competition submission was made on lagged data alone","metadata":{}},{"cell_type":"code","source":"#Copy the dataframe\ndf_lagged = target_sample.copy()\n\n# del target_sample\n\n#trailing_window_size = 2\ndf_lagged = pd.concat([df_lagged.shift(1)], axis=1)\ndf_lagged = df_lagged.fillna(0) #filling with 0's appears to have less of an effect on reducing model performance than mathematical interpolation\ndf_lagged.head()\ngc.collect()\n\n#compare original and lagged data with weekly resampling to check for any change\nrandom_autocorrelation_plot (target_sample, '1w')\n\nrandom_autocorrelation_plot (df_lagged, '1w')","metadata":{"execution":{"iopub.status.busy":"2022-03-06T16:40:22.60123Z","iopub.execute_input":"2022-03-06T16:40:22.601523Z","iopub.status.idle":"2022-03-06T16:40:25.500436Z","shell.execute_reply.started":"2022-03-06T16:40:22.60149Z","shell.execute_reply":"2022-03-06T16:40:25.499607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Differenced Data**\nThe dataframe was changed so the one-step difference in data was used across both feature and target. This is a method used to make data stationary. Predicition must be made on differenced data and the predictions inverted. We were unable to utilise this as the test_df had no corresponding time-id column to use for setting a DateTime index.\n\nSubmiting a model trained on differenced data produced a score of 0.138 which was a drop from 0.149.","metadata":{}},{"cell_type":"code","source":"def inverse_diff(actual_df, pred_df):\n    '''\n    Transforms the differentiated values back\n    Args:\n        actual dataframe: Values of the columns, numpy array of floats \n        predicted dataframe: Values of the columns, numpy array of floats \n    Returns:\n        Dataframe with the predicted values\n    '''\n    df_res = pred_df.copy()\n    columns = actual_df.columns\n    for col in columns: \n        df_res[col].replace(actual_df[col].iloc[-1] + df_res[str(col)].cumsum())\n    return df_res","metadata":{"execution":{"iopub.status.busy":"2022-03-06T16:41:38.470955Z","iopub.execute_input":"2022-03-06T16:41:38.471415Z","iopub.status.idle":"2022-03-06T16:41:38.479554Z","shell.execute_reply.started":"2022-03-06T16:41:38.471377Z","shell.execute_reply":"2022-03-06T16:41:38.478443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create a dataset from the differences per increment and pad the first row with 0's\ndifferenced_data= target_sample.diff(axis = 0, periods = 1)\ndifferenced_data=differenced_data.fillna(0)\ndifferenced_data.head()\n#del(differenced_data)\n\n#Visualise change in autocorration after differencing\nrandom_autocorrelation_plot (target_sample, '1w')\n\nrandom_autocorrelation_plot (differenced_data, '1w')","metadata":{"execution":{"iopub.status.busy":"2022-03-06T16:41:46.244078Z","iopub.execute_input":"2022-03-06T16:41:46.244378Z","iopub.status.idle":"2022-03-06T16:41:50.458505Z","shell.execute_reply.started":"2022-03-06T16:41:46.244347Z","shell.execute_reply":"2022-03-06T16:41:50.457657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Using a lagged and differenced dataframe resulted in a score of  -0.027","metadata":{}},{"cell_type":"markdown","source":"### **Resampling the Data**\nThis would result in a smaller dataframe and utilise medians across a 7 time-step lag. The theory was to reduce noise without losing variation. The autocorrelation plot is smoother but stil variable.\n","metadata":{}},{"cell_type":"code","source":"#Resample dataframe by averaging to period=7 \nfor column in target_sample.columns:\n    resampled_train=target_sample\n    resampled_train[column]=resampled_train[column].resample('W').median()\n    resampled_train[column]=resampled_train[column].interpolate(method ='linear', limit_direction ='backward')\n\n#Compare the autocorrelation in the new dataframe with original\nrandom_autocorrelation_plot (target_sample, '1w')\n\nrandom_autocorrelation_plot (resampled_train, '1w')","metadata":{"execution":{"iopub.status.busy":"2022-03-06T16:42:43.62336Z","iopub.execute_input":"2022-03-06T16:42:43.623849Z","iopub.status.idle":"2022-03-06T16:44:08.77164Z","shell.execute_reply.started":"2022-03-06T16:42:43.6238Z","shell.execute_reply":"2022-03-06T16:44:08.770771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Principal Component Analysis**\nThe goal is to reduce the dimensionality of features fed into the neural network.","metadata":{}},{"cell_type":"code","source":"# installing and importing the dependencies\n!pip install pca\nfrom pca import pca","metadata":{"execution":{"iopub.status.busy":"2022-03-06T16:49:16.10357Z","iopub.execute_input":"2022-03-06T16:49:16.10392Z","iopub.status.idle":"2022-03-06T16:49:31.248787Z","shell.execute_reply.started":"2022-03-06T16:49:16.103874Z","shell.execute_reply":"2022-03-06T16:49:31.247793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pca\ndef dim_red(df):\n    \"\"\" This function extracts the principal components of the features of the data sets\n    \"\"\"\n    features = [f'f_{i}' for i in range(300)]\n    df_ = df[features]\n    \n    pca()\n    pca_model = pca(n_components=0.98) # explained variance 90%\n    results = pca_model.fit_transform(df_)\n    \n    del df_\n    \n    return results,pca_model","metadata":{"execution":{"iopub.status.busy":"2022-03-06T16:58:20.242333Z","iopub.execute_input":"2022-03-06T16:58:20.242742Z","iopub.status.idle":"2022-03-06T16:58:20.24941Z","shell.execute_reply.started":"2022-03-06T16:58:20.242691Z","shell.execute_reply":"2022-03-06T16:58:20.248508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  the function obtains the main principal components\ntarget_sample = train.sample(frac = 0.10)\n\nresults,pca_model = dim_red(target_sample)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T16:58:22.52042Z","iopub.execute_input":"2022-03-06T16:58:22.520741Z","iopub.status.idle":"2022-03-06T17:00:24.254782Z","shell.execute_reply.started":"2022-03-06T16:58:22.520708Z","shell.execute_reply":"2022-03-06T17:00:24.253916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.keys()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T16:55:16.972454Z","iopub.execute_input":"2022-03-06T16:55:16.972819Z","iopub.status.idle":"2022-03-06T16:55:16.980357Z","shell.execute_reply.started":"2022-03-06T16:55:16.972745Z","shell.execute_reply":"2022-03-06T16:55:16.979581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot the scree plot to show the components with the most variance in the data set\nplt.figure(figsize=(10,5))\nplt.title('Cumulative Explained Variance vs Principal Components',fontweight='bold',fontsize=12)\nplt.plot(np.cumsum(results['variance_ratio']))\nplt.xlabel('Num of Components')\nplt.ylabel('Cumulative explained variance')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T17:00:24.418159Z","iopub.execute_input":"2022-03-06T17:00:24.418382Z","iopub.status.idle":"2022-03-06T17:00:24.628836Z","shell.execute_reply.started":"2022-03-06T17:00:24.418355Z","shell.execute_reply":"2022-03-06T17:00:24.627837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot components/variance\n%matplotlib inline\npca_model.plot(figsize=(10,8))\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T17:00:24.630719Z","iopub.execute_input":"2022-03-06T17:00:24.63098Z","iopub.status.idle":"2022-03-06T17:00:24.862081Z","shell.execute_reply.started":"2022-03-06T17:00:24.63095Z","shell.execute_reply":"2022-03-06T17:00:24.86124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the plots above, 250 components has an explained variance of 98 percent.The principal components are loaded into the model as inputs. The results from the model after PCA is -15.3, and therefore not suitable for this challenge. PCA reduces 50 features only, so it doesn't solve the problem of dimensionality.","metadata":{}},{"cell_type":"markdown","source":"### **Feature Engineering Summary**\n\nMore tests are needed to examine the data inorder to apply feature extraction and selection criteria to improve model performance. We were unable to leverage many feature engineering tools particularly the [scikit](https://scikit-learn.org/stable/auto_examples/applications/plot_cyclical_feature_engineering.html) library  due to computational limits arising from the size of the dataframe.","metadata":{}},{"cell_type":"code","source":"# free memory from dataframes in the previous section\ndel target_sample\ndel scaled\ndel resampled_train\ndel differenced_data\ndel df_lagged\n","metadata":{"execution":{"iopub.status.busy":"2022-03-06T17:12:01.32216Z","iopub.execute_input":"2022-03-06T17:12:01.322875Z","iopub.status.idle":"2022-03-06T17:12:01.345154Z","shell.execute_reply.started":"2022-03-06T17:12:01.322818Z","shell.execute_reply":"2022-03-06T17:12:01.344231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Model Training**\nOur baseline model is a Dense Neural Network that uses the swish non-linearity function. Swish is a smooth, non-monotonic function. The data is split into folds with StratifiedKFold from Sklearn, and later split into train and validation sets during training. The four models are trained on this data by looping over the folds. StratifiedKFold cross-validation object is a variation of KFold that returns stratified folds made by preserving the percentage of samples for each class.\n\nThe best weights of each model is saved during training and appended into a list for inference. For each fold, the model is trained for  30 epochs. Validation loss is monitored, and early stopping used used to avoid overfitting. Compute resources are optimized by deleting train and validation dataframes after every fold of training.\nValidation set is used to evaluate model performance after training. The metrics are Root mean square and Pearson correlation.","metadata":{}},{"cell_type":"code","source":"# investment ids\ninvestment_id = train[\"investment_id\"]\n\n# targets\ny = train[\"target\"]\n\n# features\ntrain = train.drop(['investment_id','time_id','target'],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **IntegerLookup layer for investment_id**\n\nThis is a preprocessing layer which maps integer features to contiguous ranges. During adapt(), the layer will analyze a data set, determine the frequency of individual integer tokens, and create a vocabulary from them.","metadata":{}},{"cell_type":"code","source":"%%time\n#IntegerLookup layer for investment_id \ninvestment_ids = list(investment_id.unique())\ninvestment_id_size = len(investment_ids) + 1\ninvestment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\ninvestment_id_lookup_layer.adapt(pd.DataFrame({\"investment_ids\":investment_ids}))","metadata":{"execution":{"iopub.status.busy":"2022-03-06T18:14:10.803146Z","iopub.execute_input":"2022-03-06T18:14:10.803479Z","iopub.status.idle":"2022-03-06T18:14:11.273878Z","shell.execute_reply.started":"2022-03-06T18:14:10.803446Z","shell.execute_reply":"2022-03-06T18:14:11.272860Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"2022-03-06 18:14:10.955982: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 342 ms, sys: 27.4 ms, total: 369 ms\nWall time: 462 ms\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **Make Tensorflow Dataset**","metadata":{}},{"cell_type":"code","source":"#data preparation for loading\n\ndef preprocess(X, y):\n    return X, y\n\ndef make_dataset(feature, investment_id, y, batch_size=1024, mode=\"train\"):\n    \n    \"\"\"The function creates a source dataset from input data.\n    Apply dataset transformations to preprocess the data.\n    Iterate over the dataset and process the elements.\n    Iteration happens in a streaming fashion, \n    so the full dataset does not need to fit into memory.\"\"\"\n    \n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature), y))\n    ds = ds.map(preprocess)\n    if mode == \"train\":\n        ds = ds.shuffle(4096)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\n\ndef t(a):\n  \"\"\"For testing: generate a float64 tensor from anything.\"\"\"\n  return tf.constant(a, dtype=tf.float64)\n\ndef tmean(x, axis=-1):\n  \"\"\"Arithmetic mean of a tensor over some axis, default last.\"\"\"\n  x = tf.convert_to_tensor(x)\n  sum = tf.reduce_sum(x, axis=axis)\n  n = tf.cast(tf.shape(x)[axis], x.dtype)\n  return sum / n\n\ndef correlationMetric(x, y, axis=-2):\n  \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n  x = tf.convert_to_tensor(x)\n  y = math_ops.cast(y, x.dtype)\n  n = tf.cast(tf.shape(x)[axis], x.dtype)\n  xsum = tf.reduce_sum(x, axis=axis)\n  ysum = tf.reduce_sum(y, axis=axis)\n  xmean = xsum / n\n  ymean = ysum / n\n  xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n  yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n  cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n  corr = cov / tf.sqrt(xvar * yvar)\n  return tf.constant(1.0, dtype=x.dtype) - corr\n\n\ndef correlationLoss(x,y, axis=-2):\n  \"\"\"Loss function that maximizes the pearson correlation coefficient between the predicted values and the labels,\n  while trying to have the same mean and variance\"\"\"\n  x = tf.convert_to_tensor(x)\n  y = math_ops.cast(y, x.dtype)\n  n = tf.cast(tf.shape(x)[axis], x.dtype)\n  xsum = tf.reduce_sum(x, axis=axis)\n  ysum = tf.reduce_sum(y, axis=axis)\n  xmean = xsum / n\n  ymean = ysum / n\n  xsqsum = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n  ysqsum = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n  cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n  corr = cov / tf.sqrt(xsqsum * ysqsum)\n  # absdif = tmean(tf.abs(x - y), axis=axis) / tf.sqrt(yvar)\n  sqdif = tf.reduce_sum(tf.math.squared_difference(x, y), axis=axis) / n / tf.sqrt(ysqsum / n)\n  # meandif = tf.abs(xmean - ymean) / tf.abs(ymean)\n  # vardif = tf.abs(xvar - yvar) / yvar\n  # return tf.convert_to_tensor( K.mean(tf.constant(1.0, dtype=x.dtype) - corr + (meandif * 0.01) + (vardif * 0.01)) , dtype=tf.float32 )\n  return tf.convert_to_tensor( K.mean(tf.constant(1.0, dtype=x.dtype) - corr + (0.01 * sqdif)) , dtype=tf.float32 )","metadata":{"execution":{"iopub.status.busy":"2022-03-06T18:14:22.343787Z","iopub.execute_input":"2022-03-06T18:14:22.344255Z","iopub.status.idle":"2022-03-06T18:14:22.366728Z","shell.execute_reply.started":"2022-03-06T18:14:22.344223Z","shell.execute_reply":"2022-03-06T18:14:22.365550Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"models = []","metadata":{"execution":{"iopub.status.busy":"2022-03-06T18:14:26.633644Z","iopub.execute_input":"2022-03-06T18:14:26.633989Z","iopub.status.idle":"2022-03-06T18:14:26.639052Z","shell.execute_reply.started":"2022-03-06T18:14:26.633956Z","shell.execute_reply":"2022-03-06T18:14:26.637600Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"#### **Model 1 - baseline model**\nThe baseline model is a dense layer comprising of ten hidden layers. Swish function introduces non linearity.  Loss optimization is Adam with a learning rate of 0.001, and the performance is evaluated with RMSE. This is common across the models forming the ensemble learning.","metadata":{}},{"cell_type":"code","source":"%%time\n\ndef dnn_baseline_model():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse, correlationMetric])\n    return model\n\n# training the model\n\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(5, shuffle=True, random_state=42)\n\nfor index, (train_indices, valid_indices) in enumerate(kfold.split(train, investment_id)):\n    X_train, X_val = train.iloc[train_indices], train.iloc[valid_indices]\n    investment_id_train = investment_id[train_indices]\n    y_train, y_val = y.iloc[train_indices], y.iloc[valid_indices]\n    investment_id_val = investment_id[valid_indices]\n    train_ds = make_dataset(X_train, investment_id_train, y_train)\n    valid_ds = make_dataset(X_val, investment_id_val, y_val, mode=\"valid\")\n    model = dnn_baseline_model()\n    checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{index}\", save_best_only=True)\n    early_stop = keras.callbacks.EarlyStopping(patience=10)\n    history = model.fit(train_ds, epochs=30, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\n    model = keras.models.load_model(f\"model_{index}\")\n    models.append(model)\n    \n    pearson_score = stats.pearsonr(model.predict(valid_ds).ravel(), y_val.values)[0]\n    print('Pearson:', pearson_score)\n    pd.DataFrame(history.history, columns=[\"mse\", \"val_mse\"]).plot()\n    plt.title(\"MSE\")\n    plt.show()\n    pd.DataFrame(history.history, columns=[\"mae\", \"val_mae\"]).plot()\n    plt.title(\"MAE\")\n    plt.show()\n    pd.DataFrame(history.history, columns=[\"rmse\", \"val_rmse\"]).plot()\n    plt.title(\"RMSE\")\n    plt.show()\n    del investment_id_train\n    del investment_id_val\n    del X_train\n    del X_val\n    del y_train\n    del y_val\n    del train_ds\n    del valid_ds\n    gc.collect()\n    break","metadata":{"execution":{"iopub.status.busy":"2022-03-06T18:14:29.994766Z","iopub.execute_input":"2022-03-06T18:14:29.995099Z","iopub.status.idle":"2022-03-06T18:45:41.993921Z","shell.execute_reply.started":"2022-03-06T18:14:29.995069Z","shell.execute_reply":"2022-03-06T18:45:41.993108Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n  UserWarning,\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30\n2455/2455 [==============================] - 152s 61ms/step - loss: 0.9107 - mse: 0.8314 - mae: 0.6243 - mape: 136914.6719 - rmse: 0.9118 - correlationMetric: 0.8687 - val_loss: 0.8391 - val_mse: 0.8372 - val_mae: 0.6306 - val_mape: 122048.3672 - val_rmse: 0.9150 - val_correlationMetric: 0.8947\n","output_type":"stream"},{"name":"stderr","text":"2022-03-06 18:17:33.486862: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/30\n2455/2455 [==============================] - 123s 50ms/step - loss: 0.8315 - mse: 0.8280 - mae: 0.6233 - mape: 150318.0469 - rmse: 0.9099 - correlationMetric: 0.8563 - val_loss: 0.8376 - val_mse: 0.8353 - val_mae: 0.6282 - val_mape: 176800.3594 - val_rmse: 0.9140 - val_correlationMetric: 0.8868\nEpoch 3/30\n2455/2455 [==============================] - 122s 50ms/step - loss: 0.8303 - mse: 0.8249 - mae: 0.6219 - mape: 148420.6562 - rmse: 0.9082 - correlationMetric: 0.8437 - val_loss: 0.8382 - val_mse: 0.8341 - val_mae: 0.6281 - val_mape: 140315.6406 - val_rmse: 0.9133 - val_correlationMetric: 0.8813\nEpoch 4/30\n2455/2455 [==============================] - 123s 50ms/step - loss: 0.8285 - mse: 0.8225 - mae: 0.6210 - mape: 149163.3281 - rmse: 0.9069 - correlationMetric: 0.8347 - val_loss: 0.8361 - val_mse: 0.8341 - val_mae: 0.6279 - val_mape: 151375.5312 - val_rmse: 0.9133 - val_correlationMetric: 0.8793\nEpoch 5/30\n2455/2455 [==============================] - 119s 48ms/step - loss: 0.8269 - mse: 0.8207 - mae: 0.6204 - mape: 141569.4219 - rmse: 0.9059 - correlationMetric: 0.8281 - val_loss: 0.8369 - val_mse: 0.8342 - val_mae: 0.6285 - val_mape: 132339.5781 - val_rmse: 0.9133 - val_correlationMetric: 0.8795\nEpoch 6/30\n2455/2455 [==============================] - 116s 47ms/step - loss: 0.8252 - mse: 0.8188 - mae: 0.6198 - mape: 142721.9844 - rmse: 0.9049 - correlationMetric: 0.8214 - val_loss: 0.8373 - val_mse: 0.8349 - val_mae: 0.6289 - val_mape: 142708.0938 - val_rmse: 0.9137 - val_correlationMetric: 0.8807\nEpoch 7/30\n2455/2455 [==============================] - 118s 48ms/step - loss: 0.8231 - mse: 0.8163 - mae: 0.6189 - mape: 144293.0156 - rmse: 0.9035 - correlationMetric: 0.8142 - val_loss: 0.8372 - val_mse: 0.8349 - val_mae: 0.6289 - val_mape: 139291.5312 - val_rmse: 0.9137 - val_correlationMetric: 0.8791\nEpoch 8/30\n2455/2455 [==============================] - 125s 51ms/step - loss: 0.8214 - mse: 0.8141 - mae: 0.6182 - mape: 142387.8125 - rmse: 0.9023 - correlationMetric: 0.8073 - val_loss: 0.8378 - val_mse: 0.8354 - val_mae: 0.6292 - val_mape: 138517.9062 - val_rmse: 0.9140 - val_correlationMetric: 0.8801\nEpoch 9/30\n2455/2455 [==============================] - 122s 50ms/step - loss: 0.8192 - mse: 0.8113 - mae: 0.6174 - mape: 144655.4688 - rmse: 0.9007 - correlationMetric: 0.7993 - val_loss: 0.8383 - val_mse: 0.8357 - val_mae: 0.6296 - val_mape: 132259.0938 - val_rmse: 0.9142 - val_correlationMetric: 0.8792\nEpoch 10/30\n2455/2455 [==============================] - 121s 49ms/step - loss: 0.8169 - mse: 0.8084 - mae: 0.6165 - mape: 147925.1406 - rmse: 0.8991 - correlationMetric: 0.7912 - val_loss: 0.8392 - val_mse: 0.8363 - val_mae: 0.6302 - val_mape: 133262.8906 - val_rmse: 0.9145 - val_correlationMetric: 0.8791\nEpoch 11/30\n2455/2455 [==============================] - 123s 50ms/step - loss: 0.8133 - mse: 0.8043 - mae: 0.6152 - mape: 155743.5938 - rmse: 0.8968 - correlationMetric: 0.7804 - val_loss: 0.8394 - val_mse: 0.8365 - val_mae: 0.6303 - val_mape: 132354.6562 - val_rmse: 0.9146 - val_correlationMetric: 0.8788\nEpoch 12/30\n2455/2455 [==============================] - 118s 48ms/step - loss: 0.8105 - mse: 0.8011 - mae: 0.6144 - mape: 161782.3438 - rmse: 0.8950 - correlationMetric: 0.7720 - val_loss: 0.8403 - val_mse: 0.8369 - val_mae: 0.6305 - val_mape: 140457.8438 - val_rmse: 0.9148 - val_correlationMetric: 0.8773\nEpoch 13/30\n2455/2455 [==============================] - 123s 50ms/step - loss: 0.8069 - mse: 0.7971 - mae: 0.6132 - mape: 167357.9844 - rmse: 0.8928 - correlationMetric: 0.7622 - val_loss: 0.8420 - val_mse: 0.8386 - val_mae: 0.6308 - val_mape: 156284.6562 - val_rmse: 0.9157 - val_correlationMetric: 0.8773\nEpoch 14/30\n2455/2455 [==============================] - 122s 50ms/step - loss: 0.8035 - mse: 0.7929 - mae: 0.6121 - mape: 174729.9688 - rmse: 0.8905 - correlationMetric: 0.7523 - val_loss: 0.8430 - val_mse: 0.8392 - val_mae: 0.6312 - val_mape: 159944.4219 - val_rmse: 0.9161 - val_correlationMetric: 0.8771\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m   raise IOError(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, compile, options)\u001b[0m\n\u001b[1;32m    132\u001b[0m   \u001b[0;31m# Recreate layers and metrics using the info stored in the metadata.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m   \u001b[0mkeras_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasObjectLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_graph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m   \u001b[0mkeras_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m   \u001b[0;31m# Generate a dictionary of all loaded nodes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mload_layers\u001b[0;34m(self, compile)\u001b[0m\n\u001b[1;32m    400\u001b[0m         self.loaded_nodes[node_metadata.node_id] = self._load_layer(\n\u001b[1;32m    401\u001b[0m             \u001b[0mnode_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             node_metadata.metadata)\n\u001b[0m\u001b[1;32m    403\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;31m# Metrics are only needed when the model is compiled later. We ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36m_load_layer\u001b[0;34m(self, node_id, identifier, metadata)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_revive_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m       \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrevive_custom_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0;31m# Add an attribute that stores the extra functions/objects saved in the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mrevive_custom_object\u001b[0;34m(identifier, metadata)\u001b[0m\n\u001b[1;32m    991\u001b[0m                      \u001b[0;34m'and `from_config` when saving. In addition, please use '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m                      \u001b[0;34m'the `custom_objects` arg when calling `load_model()`.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m                      .format(identifier))\n\u001b[0m\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Unable to restore custom object of type _tf_keras_metric currently. Please make sure that the layer implements `get_config`and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`."],"ename":"ValueError","evalue":"Unable to restore custom object of type _tf_keras_metric currently. Please make sure that the layer implements `get_config`and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.","output_type":"error"}]},{"cell_type":"markdown","source":"#### **Model 2 - dnn_10fold_model**\nThree dense layers are added to the baseline model. To avoid noise modelling and overfitting, drop out layers are added after every four dense layers. The baseline metric and training params are retained.\n","metadata":{}},{"cell_type":"code","source":"%%time\ndef dnn_10fold_model():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)    \n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n   # investment_id_x = layers.Dropout(0.65)(investment_id_x)\n   \n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dropout(0.65)(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n   # x = layers.Dropout(0.2)(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n  #  x = layers.Dropout(0.4)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.75)(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse, correlationMetric])\n    return model\n\n# training\n\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(10, shuffle=True, random_state=42)\n\nfor index, (train_indices, valid_indices) in enumerate(kfold.split(train, investment_id)):\n    X_train, X_val = train.iloc[train_indices], train.iloc[valid_indices]\n    investment_id_train = investment_id[train_indices]\n    y_train, y_val = y.iloc[train_indices], y.iloc[valid_indices]\n    investment_id_val = investment_id[valid_indices]\n    train_ds = make_dataset(X_train, investment_id_train, y_train)\n    valid_ds = make_dataset(X_val, investment_id_val, y_val, mode=\"valid\")\n    model = dnn_10fold_model()\n    checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{index}\", save_best_only=True)\n    early_stop = keras.callbacks.EarlyStopping(patience=10)\n    history = model.fit(train_ds, epochs=30, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\n    model = keras.models.load_model(f\"model_{index}\")\n    models.append(model)\n    \n    pearson_score = stats.pearsonr(model.predict(valid_ds).ravel(), y_val.values)[0]\n    print('Pearson:', pearson_score)\n    pd.DataFrame(history.history, columns=[\"mse\", \"val_mse\"]).plot()\n    plt.title(\"MSE\")\n    plt.show()\n    pd.DataFrame(history.history, columns=[\"mae\", \"val_mae\"]).plot()\n    plt.title(\"MAE\")\n    plt.show()\n    pd.DataFrame(history.history, columns=[\"rmse\", \"val_rmse\"]).plot()\n    plt.title(\"RMSE\")\n    plt.show()\n    del investment_id_train\n    del investment_id_val\n    del X_train\n    del X_val\n    del y_train\n    del y_val\n    del train_ds\n    del valid_ds\n    gc.collect()\n    break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Model 3 - dnn_baseline_model2**\nThe number of nodes in the dense layers are adjusted compared to the baseline model. Drop out layers introduced after every dense layer. Baseline params are retained.","metadata":{}},{"cell_type":"code","source":"%%time\ndef dnn_baseline_model2():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float32)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dropout(0.5)(investment_id_x)\n    investment_id_x = layers.Dense(32, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dropout(0.5)(investment_id_x)\n    #investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dropout(0.5)(feature_x)\n    feature_x = layers.Dense(128, activation='swish')(feature_x)\n    feature_x = layers.Dropout(0.5)(feature_x)\n    feature_x = layers.Dense(64, activation='swish')(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(64, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(16, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.5)(x)\n    output = layers.Dense(1)(x)\n    output = tf.keras.layers.BatchNormalization(axis=1)(output)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse, correlationMetric])\n    return model\n\n# training\n\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(5, shuffle=True, random_state=42)\n\nfor index, (train_indices, valid_indices) in enumerate(kfold.split(train, investment_id)):\n    X_train, X_val = train.iloc[train_indices], train.iloc[valid_indices]\n    investment_id_train = investment_id[train_indices]\n    y_train, y_val = y.iloc[train_indices], y.iloc[valid_indices]\n    investment_id_val = investment_id[valid_indices]\n    train_ds = make_dataset(X_train, investment_id_train, y_train)\n    valid_ds = make_dataset(X_val, investment_id_val, y_val, mode=\"valid\")\n    model = dnn_baseline_model2()\n    checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{index}\", save_best_only=True)\n    early_stop = keras.callbacks.EarlyStopping(patience=10)\n    history = model.fit(train_ds, epochs=30, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\n    model = keras.models.load_model(f\"model_{index}\")\n    models.append(model)\n    \n    pearson_score = stats.pearsonr(model.predict(valid_ds).ravel(), y_val.values)[0]\n    print('Pearson:', pearson_score)\n    pd.DataFrame(history.history, columns=[\"mse\", \"val_mse\"]).plot()\n    plt.title(\"MSE\")\n    plt.show()\n    pd.DataFrame(history.history, columns=[\"mae\", \"val_mae\"]).plot()\n    plt.title(\"MAE\")\n    plt.show()\n    pd.DataFrame(history.history, columns=[\"rmse\", \"val_rmse\"]).plot()\n    plt.title(\"RMSE\")\n    plt.show()\n    del investment_id_train\n    del investment_id_val\n    del X_train\n    del X_val\n    del y_train\n    del y_val\n    del train_ds\n    del valid_ds\n    gc.collect()\n    break\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Model 4 - LSTM Model**\nFor the first 3 models, the problem is considered as a regression problem that can be solved without factoring the effect of time. In this model, LSTM is used to to carry forward important aspects in time which might affect the future outputs. The output of the dense layer to LSTM is normalized to a mean close to zero and standard deviation of one with batch normalization.","metadata":{}},{"cell_type":"code","source":"%%time\ndef lstm_model():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(32, activation='swish')(investment_id_x)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dense(128, activation='swish')(feature_x)\n    feature_x = layers.Dense(64, activation='swish')(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dense(256, activation='swish', kernel_regularizer=\"l2\")(x)\n    # x = layers.Dense(256, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    # x = layers.Dense(16, activation='swish', kernel_regularizer=\"l2\")(x)\n\n\n    x = layers.BatchNormalization(name='batch_norm1')(x)\n    x = layers.Dense(256, activation='swish', name='dense1')(x)\n    x = layers.Dropout(0.1, name='dropout1')(x)\n    x = layers.Reshape((1, -1), name='reshape1')(x)\n    x = layers.BatchNormalization(name='batch_norm2')(x)\n    x = layers.LSTM(128, return_sequences=True, activation='relu', name='lstm1')(x)\n    x = layers.LSTM(16, return_sequences=False, activation='relu', name='lstm2')(x)\n\n\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss=correlationLoss, metrics=['mse', \"mae\", rmse, correlationMetric])\n    return model\n\n# training\n\n\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(5, shuffle=True, random_state=42)\n\nfor index, (train_indices, valid_indices) in enumerate(kfold.split(train, investment_id)):\n    X_train, X_val = train.iloc[train_indices], train.iloc[valid_indices]\n    investment_id_train = investment_id[train_indices]\n    y_train, y_val = y.iloc[train_indices], y.iloc[valid_indices]\n    investment_id_val = investment_id[valid_indices]\n    train_ds = make_dataset(X_train, investment_id_train, y_train)\n    valid_ds = make_dataset(X_val, investment_id_val, y_val, mode=\"valid\")\n    model = lstm_model()\n    checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{index}\", save_best_only=True)\n    early_stop = keras.callbacks.EarlyStopping(patience=10)\n    history = model.fit(train_ds, epochs=30, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\n    model = keras.models.load_model(f\"model_{index}\")\n    models.append(model)\n    \n    pearson_score = stats.pearsonr(model.predict(valid_ds).ravel(), y_val.values)[0]\n    print('Pearson:', pearson_score)\n    pd.DataFrame(history.history, columns=[\"mse\", \"val_mse\"]).plot()\n    plt.title(\"MSE\")\n    plt.show()\n    pd.DataFrame(history.history, columns=[\"mae\", \"val_mae\"]).plot()\n    plt.title(\"MAE\")\n    plt.show()\n    pd.DataFrame(history.history, columns=[\"rmse\", \"val_rmse\"]).plot()\n    plt.title(\"RMSE\")\n    plt.show()\n    del investment_id_train\n    del investment_id_val\n    del X_train\n    del X_val\n    del y_train\n    del y_val\n    del train_ds\n    del valid_ds\n    gc.collect()\n    break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Model Summary and Architecture**","metadata":{}},{"cell_type":"code","source":"#printing the model summary\nmodel = get_model()\n#model.summary()\n\n#desplaing the model architecture\n#keras.utils.plot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.execute_input":"2022-03-05T14:42:58.814071Z","iopub.status.busy":"2022-03-05T14:42:58.813437Z","iopub.status.idle":"2022-03-05T14:42:58.945321Z","shell.execute_reply":"2022-03-05T14:42:58.944585Z","shell.execute_reply.started":"2022-03-05T14:42:58.814033Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Inference**\nThe predicted values of the test data is an average of the predictions made by each saved model.","metadata":{}},{"cell_type":"code","source":"def preprocess_test(investment_id, feature):\n   return (investment_id, feature), 0\ndef make_test_dataset(feature, investment_id, batch_size=1024):\n   ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n   ds = ds.map(preprocess_test)\n   ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n   return ds\ndef inference(models, ds):\n   y_preds = []\n   for model in models:\n       y_pred = model.predict(ds)\n       y_preds.append(y_pred)\n   return np.mean(y_preds, axis=0)","metadata":{"execution":{"iopub.execute_input":"2022-03-05T14:43:01.948268Z","iopub.status.busy":"2022-03-05T14:43:01.947969Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Submission**","metadata":{}},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test() \nfor (test_df, sample_prediction_df) in iter_test:\n   ds = make_test_dataset(test_df[features], test_df[\"investment_id\"])\n   sample_prediction_df['target'] = inference(models, ds)\n   env.predict(sample_prediction_df) ","metadata":{"execution":{"iopub.execute_input":"2022-03-02T07:27:45.603167Z","iopub.status.busy":"2022-03-02T07:27:45.602874Z","iopub.status.idle":"2022-03-02T07:27:45.975114Z","shell.execute_reply":"2022-03-02T07:27:45.974379Z","shell.execute_reply.started":"2022-03-02T07:27:45.603127Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Results**\n","metadata":{}},{"cell_type":"markdown","source":"## **Conclusion**\n\nThe highest score achieved was 0.153, and further increment of the ensemble models couldn't improve the results. The task was considered as a regression problem that could be solved with a feed forward neural network. From the EDA, there are so many complexities and noise in the dataset which limits performance. More so, 27 percent of the data is missing. Feature engineering techniques such as PCA, interpolating data points couldn't make it better. Kernel crashes due to limited computer memory impeded training, and feature selection on the dataset. The team members were satisfied with the Kaggle leaderboard score despite the challenges and the short amount of time available for the task.","metadata":{}}]}